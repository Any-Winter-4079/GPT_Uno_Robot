Given that different tasks—such as object detection, person recognition, depth estimation, etc.—may require different models and libraries (TensorFlow, YOLOv8, MediaPipe, OpenCV, etc.), and these can have varying computing times (to detect the object, recognize the person, etc.) and require different image qualities to function correctly, it's important to find a balance between the requirements of the used models and libraries and those of the robot's task (for example, a driving task, where the scene constantly changes, with lane changes, signals, etc., may require a higher frame rate than a person recognition task when talking to an interlocutor—to refer to them by their name—, which generally will not change as quickly). Therefore, it is necessary to find an image size and quality that can be processed by the libraries with the highest possible performance—which may require a larger size or quality of frame—, but always meeting the frame rate of the robot's task.

Note that, for this reason, it is decided to include /camera_config in the ESP32-CAMs, to be able to change size and quality on-the-fly depending on the task to be completed. Likewise, other parameters, such as clock speed or number of buffers, could be made configurable in the same way, but after several tests, it is considered sufficient to work consistently with 1 buffer and at 20MHz.

As a first step, and to get an idea of the maximum possible performance by the ESP32-CAMs, the following study is carried out, requesting (synchronously) a total of 1,000 frames for each of the following 30 combinations of size and quality, and calculating their average fetch time.

Observing the results (Table 1 for M5Stack Wide and Table 2 for Ai-Thinker), it can be seen how latency varies from 0.027 to 0.618 seconds—i.e., from ~1.7 to 37 fps.
